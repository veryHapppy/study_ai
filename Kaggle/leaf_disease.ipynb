{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3KJIn2A2EbppsanMDNdZ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veryHapppy/study_ai/blob/main/Kaggle/leaf_disease.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXG8myIiF-o-"
      },
      "outputs": [],
      "source": [
        "#!pip install optuna\n",
        "#!pip install plotly\n",
        "#!pip install konlpy\n",
        "#!pip install transformers datasets accelerate\n",
        "#!pip install evaluate\n",
        "#!pip install timm\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import optuna\n",
        "import optuna.visualization as vis\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from konlpy.tag import Okt\n",
        "import urllib.request\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import evaluate\n",
        "from transformers import pipeline\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import zipfile\n",
        "import json\n",
        "import torchvision.models as models\n",
        "from torchvision.datasets import ImageFolder\n",
        "import timm\n",
        "from timm.data import Mixup\n",
        "from timm.loss import SoftTargetCrossEntropy\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import timm\n",
        "\n",
        "# 1. 'swinv2'와 '384'가 모두 포함된 모델 찾기\n",
        "models = timm.list_models('*swinv2*384*', pretrained=True)\n",
        "\n",
        "# 2. 결과 출력\n",
        "for m in models:\n",
        "    print(m)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "ciFSpUVoH2ke",
        "outputId": "04dd7578-178d-4a5b-d833-d69c06d738ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport timm\\n\\n# 1. 'swinv2'와 '384'가 모두 포함된 모델 찾기\\nmodels = timm.list_models('*swinv2*384*', pretrained=True)\\n\\n# 2. 결과 출력\\nfor m in models:\\n    print(m)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LeafDiseaseDataset(Dataset):\n",
        "    def __init__(self, root_dir, csv_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # CSV에서 이미지 ID와 견종 가져오기\n",
        "        img_id = self.df.iloc[idx, 0]\n",
        "        label = int(self.df.iloc[idx, 1])\n",
        "\n",
        "        # 이미지 경로 생성 (ID에 .jpg 붙이기)\n",
        "        img_path = os.path.join(self.root_dir, f\"{img_id}\")\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(384, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(), # 컬러 이미지에선 필수!\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# 2. 테스트/검증용 (증강 제외 - 깨끗한 원본)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(448),\n",
        "    transforms.CenterCrop(384),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# 2. 데이터셋 생성 (경로를 본인의 폴더명으로 바꾸세요)\n",
        "zip_path = '/content/drive/MyDrive/Colab Notebooks/[CNN]Cassava Leaf Disease Classification/train.zip'\n",
        "extract_path = '/content/dataset/train' # 코랩 내부 경로\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    z.extractall(extract_path)\n",
        "\n",
        "# 1. 전체 데이터셋 생성 (아까 만든 DogCatDataset 활용)\n",
        "full_dataset = LeafDiseaseDataset(root_dir='/content/dataset/train',csv_file='/content/drive/MyDrive/Colab Notebooks/[CNN]Cassava Leaf Disease Classification/train.csv', transform=None)\n",
        "\n",
        "# 2. 비율 정하기 (예: 80%는 학습용, 20%는 검증용)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "\n",
        "# 3. 무작위로 데이터 분할\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "class ApplyTransform(Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.subset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "# 이제 각각의 로더에 맞게 적용\n",
        "train_dataset = ApplyTransform(train_dataset, transform=train_transform)\n",
        "val_dataset = ApplyTransform(val_dataset, transform=test_transform)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "ze_ipoBjGFBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PetFindermodel(nn.Module):\n",
        "    def __init__(self, model_name='swinv2_base_window12to24_192to384.ms_in22k_ft_in1k', pretrained=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0)\n",
        "        num_img_features = self.backbone.num_features\n",
        "\n",
        "        self.meta_fc = nn.Sequential(\n",
        "            nn.Linear(16, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        self.combined_fc = nn.Sequential(\n",
        "            nn.Linear(num_img_features + 64, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 5) # Pawpularity 점수 (회귀)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, meta):\n",
        "        # 이미지 특징 추출\n",
        "        img_features = self.backbone(image) # (batch, 768)\n",
        "\n",
        "        # 메타데이터 특징 추출\n",
        "        meta_features = self.meta_fc(meta) # (batch, 64)\n",
        "\n",
        "        # 특징 결합 (옆으로 붙이기)\n",
        "        combined = torch.cat((img_features, meta_features), dim=1) # (batch, 768 + 64)\n",
        "\n",
        "        # 최종 점수 계산\n",
        "        output = self.combined_fc(combined)\n",
        "        return output"
      ],
      "metadata": {
        "id": "-I79kKA1HiXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval(model, train_loader, test_loader, optimizer, criterion):\n",
        "    scaler = torch.amp.GradScaler() # AMP를 위한 스케일러\n",
        "    accumulation_steps = 8 # 4번 모아서 업데이트 (배치 4 -> 16 효과)\n",
        "    # 3 에폭 정도만 학습해서 성능을 확인 (시간 절약)\n",
        "    for epoch in range(2):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss = loss / accumulation_steps\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                scaler.step(optimizer) # 업데이트\n",
        "                scaler.update() # 스케일러 갱신\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "    # 최종 검증 단계\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def objective_step1(trial):\n",
        "    # 1. 하이퍼파라미터 제안\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [2,4])\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
        "\n",
        "    # 2. 데이터 로더 설정\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                              shuffle=True, num_workers=0, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # 3. 모델 초기화\n",
        "    model_name='swinv2_base_window12to24_192to384.ms_in22k_ft_in1k'\n",
        "    model = timm.create_model(model_name, pretrained=True, num_classes=5)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    for param in model.parameters(): param.requires_grad = False\n",
        "    for param in model.head.parameters(): param.requires_grad = True\n",
        "\n",
        "    optimizer_head = optim.Adam(model.head.parameters(), lr=1e-3)\n",
        "    try:\n",
        "        model.train()\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "          if i > 30: break # 더 빨리 훑고 넘어가도록 조정\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          optimizer_head.zero_grad()\n",
        "          with torch.amp.autocast('cuda'): # AMP 적용\n",
        "              outputs = model(images)\n",
        "              loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer_head.step()\n",
        "\n",
        "        for param in model.parameters(): param.requires_grad = True\n",
        "\n",
        "        optimizer_full = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        acc = train_and_eval(model, train_loader, val_loader, optimizer_full, criterion)\n",
        "        return acc\n",
        "    except RuntimeError as e:\n",
        "        if \"out of memory\" in str(e):\n",
        "            print(f\"| OOM 발생 - Trial {trial.number} 건너뜀 |\")\n",
        "            return 0.0 # OOM 발생 시 낮은 점수를 주고 다음으로 넘어감\n",
        "        else: raise e\n",
        "    finally:\n",
        "        if 'model' in locals(): del model\n",
        "        if 'optimizer_head' in locals(): del optimizer_head\n",
        "        if 'optimizer_full' in locals(): del optimizer_full\n",
        "        if 'train_loader' in locals(): del train_loader\n",
        "        if 'val_loader' in locals(): del val_loader\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/Colab Notebooks/[CNN]Cassava Leaf Disease Classification\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# 데이터베이스 파일 경로\n",
        "db_path = os.path.join(save_dir, \"Leaf_Disease.db\")\n",
        "storage_name = f\"sqlite:///{db_path}\"\n",
        "study1 = optuna.create_study(\n",
        "    study_name=\"optimization_v2\", # 스터디 이름 지정\n",
        "    storage=storage_name,                # SQLite 파일로 저장\n",
        "    direction=\"maximize\",\n",
        "    load_if_exists=True                  # 중단됐을 때 다시 실행하면 이어서 시작함!\n",
        ")\n",
        "study1.optimize(objective_step1, n_trials=20) # 시도횟수\n",
        "\n",
        "best_lr = study1.best_params['lr']\n",
        "best_batch_size = study1.best_params['batch_size']\n",
        "best_weight_decay = study1.best_params['weight_decay']\n",
        "print(f\"1단계 완료, 최적의 lr: {best_lr}, batch_size: {best_batch_size}, weight_decay: {best_weight_decay}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XFhzk8ijHRvD",
        "outputId": "f1038a62-aadd-46bf-ff47-505a1607f618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-08 08:48:30,351] Using an existing study with name 'optimization_v2' instead of creating a new one.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-08 10:07:58,780] Trial 4 finished with value: 88.0607476635514 and parameters: {'batch_size': 4, 'lr': 0.00014597217243992892, 'weight_decay': 1.1070759528758532e-05}. Best is trial 4 with value: 88.0607476635514.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-08 11:27:08,641] Trial 5 finished with value: 78.69158878504673 and parameters: {'batch_size': 4, 'lr': 0.0004402954738575345, 'weight_decay': 8.070064192352575e-05}. Best is trial 4 with value: 88.0607476635514.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-08 12:57:15,968] Trial 6 finished with value: 79.92990654205607 and parameters: {'batch_size': 2, 'lr': 0.0001409649636458517, 'weight_decay': 6.770169185954349e-06}. Best is trial 4 with value: 88.0607476635514.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/[CNN]Cassava Leaf Disease Classification/best_params_step1.json', 'w') as f:\n",
        "    json.dump(study1.best_params, f)"
      ],
      "metadata": {
        "id": "bRYNZrJiMuOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 그 레시피대로 '진짜 최종 모델'을 생성합니다.\n",
        "model_name='swinv2_base_window12to24_192to384.ms_in22k_ft_in1k'\n",
        "final_model = timm.create_model(model_name, pretrained=True, num_classes=5)\n",
        "final_model = final_model.to(device)\n",
        "\n",
        "history = {'train_loss': [], 'val_acc': []}\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_dataset = ApplyTransform(train_dataset, transform=train_transform)\n",
        "val_dataset = ApplyTransform(val_dataset, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=best_batch_size,\n",
        "                          shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "optimizer = optim.Adam(final_model.head.parameters(), lr=1e-3)\n",
        "\n",
        "mixup_fn = Mixup(\n",
        "    mixup_alpha=0.8,\n",
        "    cutmix_alpha=1.0,\n",
        "    prob=0.5,        # 50% 확률로 섞음\n",
        "    switch_prob=0.5,\n",
        "    mode='batch',\n",
        "    label_smoothing=0.1,\n",
        "    num_classes=5\n",
        ")\n",
        "criterion_mixup = SoftTargetCrossEntropy()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "for param in final_model.parameters(): param.requires_grad = False\n",
        "for param in final_model.head.parameters(): param.requires_grad = True\n",
        "\n",
        "print(\"Starting Phase 1...\")\n",
        "for epoch in range(5):\n",
        "    final_model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = final_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Phase 1 - Epoch [{epoch+1}/5] Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "for param in final_model.parameters(): param.requires_grad = True\n",
        "\n",
        "optimizer = optim.Adam(final_model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)\n",
        "scaler = torch.cuda.amp.GradScaler() # AMP를 위한 스케일러\n",
        "accumulation_steps = 8\n",
        "best_acc = 0.0\n",
        "epochs = 30\n",
        "for epoch in range(epochs):\n",
        "    final_model.train()\n",
        "    total_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        images, mix_labels = mixup_fn(images, labels)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            outputs = final_model(images)\n",
        "            loss = criterion_mixup(outputs, mix_labels)\n",
        "            loss = loss / accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "        total_loss += (loss.item() * accumulation_steps)\n",
        "\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer) # 업데이트\n",
        "            scaler.update() # 스케일러 갱신\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "# 최종 검증 단계\n",
        "    final_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = final_model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    scheduler.step(accuracy)\n",
        "    history['train_loss'].append(avg_loss)\n",
        "    history['val_acc'].append(accuracy)\n",
        "\n",
        "    if accuracy > best_acc:\n",
        "        best_acc = accuracy\n",
        "\n",
        "    checkpoint = {\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': final_model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'best_acc': best_acc,\n",
        "    }\n",
        "    # 구글 드라이브 경로에 'last_checkpoint.pth'로 저장\n",
        "    torch.save(checkpoint, '/content/drive/MyDrive/Colab Notebooks/[CNN]Cassava Leaf Disease Classification/last_checkpoint.pth')\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}, Acc: {accuracy:.2f}%\")\n",
        "\n",
        "print(\"최종 모델 학습 완료\")"
      ],
      "metadata": {
        "id": "rz1sHS19Mx9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "model_name='swinv2_base_window12to24_192to384.ms_in22k_ft_in1k'\n",
        "final_model = timm.create_model(model_name, pretrained=True, num_classes=5)\n",
        "final_model = final_model.to(device)\n",
        "\n",
        "history = {'train_loss': [], 'val_acc': []}\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_dataset = ApplyTransform(train_dataset, transform=train_transform)\n",
        "val_dataset = ApplyTransform(val_dataset, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=best_batch_size,\n",
        "                          shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "optimizer = optim.Adam(final_model.head.parameters(), lr=1e-3)\n",
        "\n",
        "mixup_fn = Mixup(\n",
        "    mixup_alpha=0.8,\n",
        "    cutmix_alpha=1.0,\n",
        "    prob=0.5,        # 50% 확률로 섞음\n",
        "    switch_prob=0.5,\n",
        "    mode='batch',\n",
        "    label_smoothing=0.1,\n",
        "    num_classes=5\n",
        ")\n",
        "criterion_mixup = SoftTargetCrossEntropy()\n",
        "optimizer = optim.Adam(final_model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)\n",
        "scaler = torch.cuda.amp.GradScaler() # AMP를 위한 스케일러\n",
        "accumulation_steps = 8\n",
        "best_acc = 0.0\n",
        "\n",
        "load_path = '/content/drive/MyDrive/Colab Notebooks/[CNN]Cassava Leaf Disease Classification/last_checkpoint.pth'\n",
        "\n",
        "if os.path.exists(load_path):\n",
        "    checkpoint = torch.load(load_path)\n",
        "    final_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    best_acc = checkpoint['best_acc']\n",
        "    history = checkpoint['history']\n",
        "    print(f\"[{start_epoch+1}] 에폭부터 이어서 시작합니다. 현재 최고 정확도: {best_acc:.2f}%\")\n",
        "else:\n",
        "    start_epoch = 0\n",
        "    print(\"처음부터 학습을 시작합니다.\")\n",
        "\n",
        "epochs = 30\n",
        "for epoch in range(epochs):\n",
        "    final_model.train()\n",
        "    total_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        images, mix_labels = mixup_fn(images, labels)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            outputs = final_model(images)\n",
        "            loss = criterion_mixup(outputs, mix_labels)\n",
        "            loss = loss / accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "        total_loss += (loss.item() * accumulation_steps)\n",
        "\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer) # 업데이트\n",
        "            scaler.update() # 스케일러 갱신\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "# 최종 검증 단계\n",
        "    final_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = final_model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    scheduler.step(accuracy)\n",
        "    history['train_loss'].append(avg_loss)\n",
        "    history['val_acc'].append(accuracy)\n",
        "\n",
        "    if accuracy > best_acc:\n",
        "        best_acc = accuracy\n",
        "\n",
        "    checkpoint = {\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': final_model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'best_acc': best_acc,\n",
        "    }\n",
        "    # 구글 드라이브 경로에 'last_checkpoint.pth'로 저장\n",
        "    torch.save(checkpoint, '/content/drive/MyDrive/Colab Notebooks/[CNN]Cassava Leaf Disease Classification/last_checkpoint.pth')\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}, Acc: {accuracy:.2f}%\")\n",
        "\n",
        "print(\"최종 모델 학습 완료\")\n",
        "    '''"
      ],
      "metadata": {
        "id": "anhkkbtmX3Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/MyDrive/Colab Notebooks/[CNN]Cassava Leaf Disease Classification'\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "# 모델 저장 (가장 성능이 좋았을 때 실행)\n",
        "torch.save(final_model.state_dict(), f\"{save_path}/final_model.pth\")"
      ],
      "metadata": {
        "id": "TdEB-HhLMzpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeafTestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        # test 폴더 내의 모든 이미지 파일 목록 (정렬하여 순서 보장)\n",
        "        self.file_list = sorted([f for f in os.listdir(root_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.file_list[idx]\n",
        "        img_path = os.path.join(self.root_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name # 이미지와 파일명을 함께 반환\n",
        "\n",
        "def generate_submission(model, test_loader, save_path):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_ids = []\n",
        "    with torch.no_grad():\n",
        "        for images, img_names in test_loader:\n",
        "            images = images.to(device)\n",
        "\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = model(images) # [batch_size, 5] 형태의 출력\n",
        "\n",
        "            # 5개 클래스 중 가장 확률이 높은 인덱스(0~4)를 선택\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_ids.extend(img_names)\n",
        "\n",
        "\n",
        "    # 4. 데이터프레임 생성 (id 컬럼 + 120개 품종 컬럼)\n",
        "    submission = pd.DataFrame({\n",
        "        'image_id': all_ids,\n",
        "        'label': all_preds\n",
        "    })\n",
        "\n",
        "    submission.to_csv(save_path, index=False)\n",
        "    print(f\"제출 파일(확률 포함)이 저장되었습니다: {save_path}\")\n",
        "\n",
        "# --- 실행 부분 ---\n",
        "\n",
        "# 1. 테스트 데이터 압축 해제 (이미 하셨다면 생략)\n",
        "test_zip_path = '/content/drive/MyDrive/Colab Notebooks/[CNN]Cassava Leaf Disease Classification/test.zip'\n",
        "with zipfile.ZipFile(test_zip_path, 'r') as z:\n",
        "    z.extractall('/content/dataset/test_data')\n",
        "\n",
        "# 2. 테스트 로더 설정 (test_transform 사용)\n",
        "test_dataset = LeafTestDataset(root_dir='/content/dataset/test_data', transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# 4. 제출 파일 생성\n",
        "generate_submission(final_model, test_loader, '/content/drive/MyDrive/Colab Notebooks/[CNN]Cassava Leaf Disease Classification/submission.csv')"
      ],
      "metadata": {
        "id": "CP5CruvNS4n7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}