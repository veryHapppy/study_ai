{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wicMORl9nyF0",
        "gZ6iBQ_an31h",
        "mib4S4PTDQTx",
        "ccUkPW5Vuqa8",
        "CT5DiZssD4tx",
        "c0XQFzHOSWL4",
        "Tlof5WImh-fe",
        "IGCs2gKS048R",
        "UlTzawq6IisV",
        "OnpL5ZWDU0jg"
      ],
      "gpuType": "T4",
      "mount_file_id": "1tGK6ym7CRQ6ClF095oP4zRcQAXmvFyVX",
      "authorship_tag": "ABX9TyM2JnqIQL22RABAeDr/J702",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veryHapppy/study_ai/blob/main/deeplearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ê°œë…"
      ],
      "metadata": {
        "id": "wicMORl9nyF0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmgApwZEibWh"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "MLP (multi-layer Perceptron)\n",
        "ìˆ˜ì¹˜ ë°ì´í„°ë“¤ì„ ì—°ê²°í•˜ë©° í•™ìŠµ\n",
        "ì…ë ¥ì¸µ : í”¼ì²˜ë“¤ì´ ë“¤ì–´ì˜¤ëŠ” ê³³\n",
        "ì€ë‹‰ì¸µ : ì •ë³´ì— ê°€ì¤‘ì¹˜ ê³±í•˜ê³  ë”í•˜ê³  íŒ¨í„´ ì¶”ì¶œí•˜ëŠ” ê³³\n",
        "ì¶œë ¥ì¸µ : ìµœì¢… ì •ë‹µ ì¶œë ¥\n",
        "ëª¨ë“  ë°ì´í„°ë¥¼ ìˆ˜ì¹˜ì  ê³¡ì„  ê´€ê³„ë¡œ ì´í•´ -> ìŠ¤ì¼€ì¼ë§ì´ í•„ìˆ˜\n",
        "\n",
        "í™œì„±í™” í•¨ìˆ˜ : ì¶œë ¥ì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ (ì‹œê·¸ëª¨ì´ë“œ, ReLU, tanh ë“±)\n",
        "\n",
        "í¼ì…‰íŠ¸ë¡  : ë”¥ëŸ¬ë‹ì˜ ìµœì†Œ ë‹¨ìœ„ (ë‰´ëŸ°ì˜ ë™ì‘ ìœˆë¦¬ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ë§Œë“  ì•Œê³ ë¦¬ì¦˜)\n",
        "y = w * x + b\n",
        "\n",
        "ì†ì‹¤í•¨ìˆ˜ : lossë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ (ì´ì§„ ë¶„ë¥˜ì—ì„œëŠ” BCELoss)\n",
        "\n",
        "ì—­ì „íŒŒ : ì¶œë ¥ì¸µì—ì„œ ë°œìƒí•œ lossë¥¼ ê±°ìŠ¬ëŸ¬ê°€ë©° ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆ\n",
        "\n",
        "ì˜µí‹°ë§ˆì´ì € : ê°€ì¤‘ì¹˜ë¥¼ ì–¼ë§ˆë‚˜, ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ë°”ê¿€ì§€ ê²°ì •í•˜ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜ (Adam)\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import"
      ],
      "metadata": {
        "id": "KKbA81Buuk90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!pip install plotly\n",
        "!pip install konlpy\n",
        "!pip install transformers datasets accelerate\n",
        "!pip install evaluate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import optuna\n",
        "import optuna.visualization as vis\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from konlpy.tag import Okt\n",
        "import urllib.request\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import evaluate\n",
        "from transformers import pipeline\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SM3Gf70xmZUq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "gZ6iBQ_an31h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# 2. ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# DataLoader: ë°ì´í„°ë¥¼ 64ê°œì”© ìª¼ê°œì„œ ëª¨ë¸ì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# 3. ëª¨ë¸ ì„¤ê³„ (ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ , MLP)\n",
        "class MNIST_MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_MLP, self).__init__()\n",
        "        # MNIST ì´ë¯¸ì§€ëŠ” 28x28 = 784ê°œì˜ í”½ì…€ì…ë‹ˆë‹¤.\n",
        "        self.fc1 = nn.Linear(784, 128) # ì…ë ¥ì¸µ -> ì€ë‹‰ì¸µ 1\n",
        "        self.fc2 = nn.Linear(128, 64)  # ì€ë‹‰ì¸µ 1 -> ì€ë‹‰ì¸µ 2\n",
        "        self.fc3 = nn.Linear(64, 10)   # ì€ë‹‰ì¸µ 2 -> ì¶œë ¥ì¸µ (0~9ê¹Œì§€ 10ê°œ ë¶„ë¥˜)\n",
        "        self.relu = nn.ReLU()          # í™œì„±í™” í•¨ìˆ˜\n",
        "\n",
        "    def forward(self, x):\n",
        "        # xì˜ í˜•íƒœ: (ë°°ì¹˜ì‚¬ì´ì¦ˆ, 1, 28, 28) -> (ë°°ì¹˜ì‚¬ì´ì¦ˆ, 784)ë¡œ í‰íƒ„í™”í•©ë‹ˆë‹¤.\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x) # ë§ˆì§€ë§‰ì€ CrossEntropyLossì—ì„œ Softmaxë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„  ìƒëµí•©ë‹ˆë‹¤.\n",
        "        return x # CrossEntropyLossê°€ ì•„ë‹Œ ê²½ìš° : torch.softmax(x, dim=1)\n",
        "\n",
        "model = MNIST_MLP()\n",
        "\n",
        "# 4. ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €\n",
        "criterion = nn.CrossEntropyLoss() # ë‹¤ì¤‘ ë¶„ë¥˜ìš© ì†ì‹¤ í•¨ìˆ˜\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # ë˜‘ë˜‘í•œ ìš´ì „ìˆ˜\n",
        "\n",
        "# 5. í•™ìŠµ ë£¨í”„ (ê°„ëµ ë²„ì „)\n",
        "for epoch in range(5): # 5ë²ˆ ë°˜ë³µ í•™ìŠµ\n",
        "    for images, labels in train_loader:\n",
        "        # ìˆœì „íŒŒ\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # ì—­ì „íŒŒ ë° ìµœì í™”\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/5], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "qqoi7N-NnDsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜ (Dropoutì´ë‚˜ Batch Normalization ë“±ì´ ìˆìœ¼ë©´ í‰ê°€ìš©ìœ¼ë¡œ ë™ì‘í•¨)\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# 2. ê¸°ìš¸ê¸° ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ)\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # ì´ë¯¸ì§€ë¥¼ í•œ ì¤„ë¡œ í´ì„œ ëª¨ë¸ì— ì…ë ¥\n",
        "        outputs = model(images)\n",
        "\n",
        "        # outputsì—ëŠ” 10ê°œ ìˆ«ìì— ëŒ€í•œ ì ìˆ˜(Logits)ê°€ ë“¤ì–´ìˆìŒ\n",
        "        # ê·¸ ì¤‘ ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ ì¸ë±ìŠ¤ê°€ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # ì „ì²´ ê°œìˆ˜ì™€ ë§íŒ ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'10,000ì¥ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ëª¨ë¸ì˜ ì •í™•ë„: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "id": "LM7P0cAMtbII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ë°ì´í„° ì¦ê°•"
      ],
      "metadata": {
        "id": "mib4S4PTDQTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° ì¦ê°• : í•œì •ëœ ë°ì´í„°ë¥¼ ì¸ìœ„ì ìœ¼ë¡œ ë³€í™”ì‹œí‚´, í•™ìŠµë°ì´í„°ë¥¼ ëŠ˜ë¦¼\n",
        "transform = transforms.Compose([\n",
        "    #transforms.RandomHorizontalFlip(p=0.5), # 50% í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „\n",
        "    transforms.RandomRotation(degrees=15),  # 15ë„ ë‚´ì™¸ë¡œ ë¬´ì‘ìœ„ íšŒì „\n",
        "    transforms.ColorJitter(brightness=0.2), # ë°ê¸° ì¡°ì ˆ\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ],
      "metadata": {
        "id": "IYFovv_cDTbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "ccUkPW5Vuqa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLPëŠ” ì´ë¯¸ì§€ë¥¼ 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜\n",
        "# CNN(í•©ì„±ê³± ì‹ ê²½ë§)ì€ 2ì°¨ì›ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ íŠ¹ì§•ì„ ì°¾ìŒ\n",
        "# í•„í„° : ì´ë¯¸ì§€ë¥¼ ëŒì•„ë‹¤ë‹ˆë©° íŠ¹ì§•ì„ ì°¾ìŒ -> í•´ë‹¹ í•„í„°ì˜ ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµ\n",
        "# í•„í„°ì˜ ê°œìˆ˜ = íŠ¹ì§•ì˜ ê°œìˆ˜\n",
        "# ìŠ¤íŠ¸ë¼ì´ë“œ : í•„í„°ê°€ í•œë²ˆì— ì´ë™í•˜ëŠ” ì¹¸ ìˆ˜\n",
        "# íŒ¨ë”© : ê°€ì¥ìë¦¬ì— 0ì„ ì±„ì›Œ ë„£ìŒ\n",
        "# í’€ë§ : íŠ¹ì§• ì¤‘ì—ì„œ ê°€ì¥ ê°•í•œ íŠ¹ì§•ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ë²„ë ¤ì„œ ë°ì´í„° í¬ê¸°ë¥¼ ì¤„ì„ (Max Pooling)\n",
        "\n",
        "# dropout : ë¬´ì‘ìœ„ë¡œ ë‰´ëŸ°ì˜ ì¼ë¶€ë¥¼ êº¼ë²„ë¦¼\n",
        "# ë°°ì¹˜ ì •ê·œí™” : layerë¥¼ ì§€ë‚  ë•Œë§ˆë‹¤ ë°ì´í„°ë¥¼ ì •ê·œí™” ì‹œí‚´ (í•™ìŠµì†ë„ ë¹¨ë¼ì§, ReLUì˜ ë‹¨ì  ì»¤)"
      ],
      "metadata": {
        "id": "9d0FMCNauskj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNIST_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_CNN, self).__init__()\n",
        "        # ì²« ë²ˆì§¸ íŠ¹ì§• ì¶”ì¶œ ì¸µ: 1ì±„ë„(í‘ë°±) ì…ë ¥ -> 16ê°œ í•„í„° ì‚¬ìš© (3x3 í¬ê¸°)\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        # ë‘ ë²ˆì§¸ íŠ¹ì§• ì¶”ì¶œ ì¸µ: 16ê°œ ì±„ë„ ì…ë ¥ -> 32ê°œ í•„í„° ì‚¬ìš©\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        # ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ëŠ” í’€ë§ ì¸µ\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # ë§ˆì§€ë§‰ ë¶„ë¥˜ ì¸µ (Fully Connected)\n",
        "        # 28x28 ì´ë¯¸ì§€ê°€ í’€ë§ì„ 2ë²ˆ ê±°ì¹˜ë©´ 7x7 í¬ê¸°ê°€ ë©ë‹ˆë‹¤. (28->14->7)\n",
        "        # ìµœì¢… ì±„ë„ ìˆ˜(32) * 7 * 7 = 1568\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10) # 0~9ê¹Œì§€ 10ê°œ ë¶„ë¥˜\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv -> ReLU -> Pool\n",
        "        x = self.pool(F.relu(self.conv1(x))) # (batch, 16, 14, 14)\n",
        "        x = self.pool(F.relu(self.conv2(x))) # (batch, 32, 7, 7)\n",
        "\n",
        "        # í•œ ì¤„ë¡œ í´ê¸° (Flatten)\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "\n",
        "        # ë¶„ë¥˜ê¸° í†µê³¼\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì¥ì¹˜: {device}\")\n",
        "model = MNIST_CNN().to(device)\n",
        "\n",
        "# 3. ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n"
      ],
      "metadata": {
        "id": "e5wl19mKvujO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(5):\n",
        "    # --- [Train Phase] ---\n",
        "    model.train() # í•™ìŠµ ëª¨ë“œ\n",
        "    train_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # --- [Validation Phase] ---\n",
        "    model.eval() # í‰ê°€ ëª¨ë“œ (ì¤‘ìš”!)\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad(): # ê¸°ìš¸ê¸° ê³„ì‚° ë” (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # ì •í™•ë„ ê³„ì‚°\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # ì—í­ë³„ ê²°ê³¼ ì¶œë ¥\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_val_loss = val_loss / len(test_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(accuracy)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/5] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "s9_HQ01MynDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32) # ë°°ì¹˜ ì •ê·œí™” ì¶”ê°€\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # ë“œë¡­ì•„ì›ƒ ì¶”ê°€ (íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ê¸‰ê²©íˆ ì¦ê°€í•˜ê¸° ë•Œë¬¸ì— ê³¼ì í•©ì´ ì¼ì–´ë‚  ìˆ˜ ìˆìŒ)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv -> BN -> ReLU -> Pool ìˆœì„œ\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x) # í•™ìŠµ ì¤‘ì—ë§Œ ë‰´ëŸ°ì„ ë•ë‹ˆë‹¤\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "hds4o3c-Brng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2D CNN optuna"
      ],
      "metadata": {
        "id": "CT5DiZssD4tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNIST_CNN(nn.Module):\n",
        "    def __init__(self, conv1_filters, conv2_filters, dropout_p):\n",
        "        super(MNIST_CNN, self).__init__()\n",
        "        # ì²« ë²ˆì§¸ Conv: 1ì±„ë„ -> ì„¤ì •ëœ í•„í„° ìˆ˜\n",
        "        self.conv1 = nn.Conv2d(1, conv1_filters, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(conv1_filters)\n",
        "        # ë‘ ë²ˆì§¸ Conv: ì²« ë²ˆì§¸ í•„í„° ìˆ˜ -> ë‘ ë²ˆì§¸ í•„í„° ìˆ˜\n",
        "        self.conv2 = nn.Conv2d(conv1_filters, conv2_filters, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(conv2_filters)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # MaxPoolì„ ë‘ ë²ˆ ê±°ì¹˜ë©´ 28x28 ì´ë¯¸ì§€ëŠ” 7x7ì´ ë©ë‹ˆë‹¤. (28 -> 14 -> 7)\n",
        "        # ë”°ë¼ì„œ Linear ì¸µì˜ ì…ë ¥ì€ (ë§ˆì§€ë§‰ í•„í„° ìˆ˜ * 7 * 7)ì…ë‹ˆë‹¤.\n",
        "        self.fc1 = nn.Linear(conv2_filters * 7 * 7, 128)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x)))) # 14x14\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x)))) # 7x7\n",
        "        x = x.view(x.size(0), -1) # í´ì£¼ê¸°\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "OL2RBwtvFueq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. í•™ìŠµìš© (ì¦ê°• í¬í•¨)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(degrees=10), # MNISTëŠ” 10ë„ ì •ë„ê°€ ì ë‹¹í•©ë‹ˆë‹¤.\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# 2. í…ŒìŠ¤íŠ¸/ê²€ì¦ìš© (ì¦ê°• ì œì™¸ - ê¹¨ë—í•œ ì›ë³¸)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# ë°ì´í„°ì…‹ ì ìš©\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=train_transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "def train_and_eval(model, train_loader, test_loader, optimizer, criterion):\n",
        "    # 3 ì—í­ ì •ë„ë§Œ í•™ìŠµí•´ì„œ ì„±ëŠ¥ì„ í™•ì¸ (ì‹œê°„ ì ˆì•½)\n",
        "    for epoch in range(2):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # ìµœì¢… ê²€ì¦ ë‹¨ê³„\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def objective_step1(trial):\n",
        "    # 1. í•™ìŠµ í™˜ê²½ ë³€ìˆ˜ë§Œ ì œì•ˆë°›ìŒ\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
        "\n",
        "    # ë°ì´í„° ë¡œë” ì„¤ì •\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # ëª¨ë¸ì€ ê¸°ë³¸ êµ¬ì¡°ë¡œ ê³ ì •\n",
        "    model = MNIST_CNN(\n",
        "        conv1_filters=16,\n",
        "        conv2_filters=32,\n",
        "        dropout_p=0.5\n",
        "    ).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # í•™ìŠµ (1~2 ì—í­ë§Œ ì§§ê²Œ ìˆ˜í–‰)\n",
        "    acc = train_and_eval(model, train_loader, test_loader, optimizer, criterion)\n",
        "    return acc\n",
        "\n",
        "# 1ë‹¨ê³„ ì‹¤í–‰\n",
        "study1 = optuna.create_study(direction=\"maximize\")\n",
        "study1.optimize(objective_step1, n_trials=25) # ì‹œë„íšŸìˆ˜\n",
        "\n",
        "best_lr = study1.best_params['lr']\n",
        "best_batch_size = study1.best_params['batch_size']\n",
        "best_weight_decay = study1.best_params['weight_decay']\n",
        "print(f\"1ë‹¨ê³„ ì™„ë£Œ, ìµœì ì˜ lr: {best_lr}, batch_size: {best_batch_size}, weight_decay: {best_weight_decay}\")"
      ],
      "metadata": {
        "id": "CzlKaU_bE6a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_step2(trial):\n",
        "    # 1ë‹¨ê³„ì—ì„œ ì°¾ì€ ìµœì ê°’ ê³ ì •\n",
        "    lr = best_lr\n",
        "    batch_size = best_batch_size\n",
        "    weight_decay = best_weight_decay\n",
        "    # ğŸ” í•„í„° ìˆ˜(ì±„ë„ ìˆ˜) íƒìƒ‰ ê³µê°„ ì„¤ì •\n",
        "    # 16ë¶€í„° 128ê¹Œì§€ 16 ë‹¨ìœ„ë¡œ íƒìƒ‰\n",
        "    n_filters_conv1 = trial.suggest_int(\"n_filters_conv1\", 16, 64, step=16)\n",
        "    n_filters_conv2 = trial.suggest_int(\"n_filters_conv2\", 32, 128, step=16)\n",
        "    dropout_p = trial.suggest_float(\"dropout_p\", 0.2, 0.5)\n",
        "\n",
        "    # ë°ì´í„° ë¡œë”\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # ëª¨ë¸ ìƒì„± (í•„í„° ìˆ˜ë¥¼ ì¸ìë¡œ ì „ë‹¬)\n",
        "    model = MNIST_CNN(\n",
        "        conv1_filters=n_filters_conv1,\n",
        "        conv2_filters=n_filters_conv2,\n",
        "        dropout_p=dropout_p\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=best_weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # í•™ìŠµ ë° ì„±ì  ì¸¡ì •\n",
        "    acc = train_and_eval(model, train_loader, test_loader, optimizer, criterion)\n",
        "    return acc\n",
        "\n",
        "# 2ë‹¨ê³„ ì‹¤í–‰\n",
        "study2 = optuna.create_study(direction=\"maximize\")\n",
        "study2.optimize(objective_step2, n_trials=15)\n",
        "\n",
        "print(f\"2ë‹¨ê³„ ì™„ë£Œ, ìµœì ì˜ êµ¬ì¡°: {study2.best_params}\")"
      ],
      "metadata": {
        "id": "eTUXVDnBD-B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = study2.best_params\n",
        "\n",
        "# 2. ê·¸ ë ˆì‹œí”¼ëŒ€ë¡œ 'ì§„ì§œ ìµœì¢… ëª¨ë¸'ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "final_model = MNIST_CNN(\n",
        "    conv1_filters=best_params['n_filters_conv1'],\n",
        "    conv2_filters=best_params['n_filters_conv2'],\n",
        "    dropout_p=best_params['dropout_p']\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
        "optimizer = optim.Adam(final_model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"ìµœì¢… ëª¨ë¸ êµ¬ì„± ì™„ë£Œ\")\n",
        "\n",
        "history = {'train_loss': [], 'val_acc': []}\n",
        "\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    final_model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = final_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # ì—í­ë§ˆë‹¤ ê²€ì¦ (í…ŒìŠ¤íŠ¸ì…‹ í™œìš©)\n",
        "    final_model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = final_model(images)\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            correct += (pred == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct / len(test_dataset)\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    history['train_loss'].append(avg_loss)\n",
        "    history['val_acc'].append(acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}, Acc: {acc:.2f}%\")\n",
        "\n",
        "print(\"ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "3S8MGNzBJCF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#í…ŒìŠ¤íŠ¸\n",
        "\n",
        "def predict_my_number(image_path, model):\n",
        "    # 1. ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬\n",
        "    img = Image.open(image_path).convert('L') # í‘ë°± ë³€í™˜\n",
        "    img = img.resize((28, 28)) # 28x28 í¬ê¸° ì¡°ì ˆ\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device) # ë°°ì¹˜ ì°¨ì› ì¶”ê°€\n",
        "\n",
        "    # ë§Œì•½ ë°°ê²½ì´ í°ìƒ‰ì´ê³  ê¸€ìê°€ ê²€ì€ìƒ‰ì´ë¼ë©´ ë°˜ì „ì‹œì¼œì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
        "    img_tensor = 1.0 - img_tensor\n",
        "\n",
        "    # 2. ì˜ˆì¸¡\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(img_tensor)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        prob = torch.nn.functional.softmax(output, dim=1) # í™•ë¥ ê°’ í™•ì¸\n",
        "\n",
        "    print(f\"ëª¨ë¸ì˜ ì˜ˆì¸¡: {predicted.item()}\")\n",
        "    print(f\"í™•ë¥ : {prob[0][predicted.item()]*100:.2f}%\")\n",
        "\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "# predict_my_number('ë³¸ì¸ì˜_íŒŒì¼_ê²½ë¡œ.png', final_model)"
      ],
      "metadata": {
        "id": "x4WwVcnJW2ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2D CNN (color) optuna"
      ],
      "metadata": {
        "id": "c0XQFzHOSWL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorCNN(nn.Module):\n",
        "    def __init__(self, conv1_filters, conv2_filters, conv3_filters, dropout_p):\n",
        "        super(ColorCNN, self).__init__()\n",
        "        # ì…ë ¥ ì±„ë„ 3ìœ¼ë¡œ ë³€ê²½\n",
        "        self.conv1 = nn.Conv2d(3, conv1_filters, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(conv1_filters)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(conv1_filters, conv2_filters, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(conv2_filters)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(conv2_filters, conv3_filters, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(conv3_filters)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # CIFAR-10ì€ ë³´í†µ 32x32 í¬ê¸°ì´ë¯€ë¡œ, í’€ë§ 2ë²ˆ ê±°ì¹˜ë©´ 8x8ì´ ë©ë‹ˆë‹¤.\n",
        "        # (32 -> 16 -> 8)\n",
        "        self.fc1 = nn.Linear(conv3_filters * 4 * 4, 256) # ì…ë ¥ í¬ê¸° ì£¼ì˜!\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "PRVLYXi7Sdpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. í•™ìŠµìš© (ì¦ê°• í¬í•¨)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(), # ì»¬ëŸ¬ ì´ë¯¸ì§€ì—ì„  í•„ìˆ˜!\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# 2. í…ŒìŠ¤íŠ¸/ê²€ì¦ìš© (ì¦ê°• ì œì™¸ - ê¹¨ë—í•œ ì›ë³¸)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# ë°ì´í„°ì…‹ ì ìš©\n",
        "train_color_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "test_color_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "test_loader = DataLoader(test_color_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "def train_and_eval(model, train_loader, test_loader, optimizer, criterion):\n",
        "    # 3 ì—í­ ì •ë„ë§Œ í•™ìŠµí•´ì„œ ì„±ëŠ¥ì„ í™•ì¸ (ì‹œê°„ ì ˆì•½)\n",
        "    for epoch in range(2):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # ìµœì¢… ê²€ì¦ ë‹¨ê³„\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def objective_step1(trial):\n",
        "    # 1. í•™ìŠµ í™˜ê²½ ë³€ìˆ˜ë§Œ ì œì•ˆë°›ìŒ\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
        "\n",
        "    # ë°ì´í„° ë¡œë” ì„¤ì •\n",
        "    train_loader = DataLoader(train_color_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # ëª¨ë¸ì€ ê¸°ë³¸ êµ¬ì¡°ë¡œ ê³ ì •\n",
        "    model = ColorCNN(\n",
        "        conv1_filters=32,\n",
        "        conv2_filters=64,\n",
        "        conv3_filters=128,\n",
        "        dropout_p=0.5\n",
        "    ).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # í•™ìŠµ (1~2 ì—í­ë§Œ ì§§ê²Œ ìˆ˜í–‰)\n",
        "    acc = train_and_eval(model, train_loader, test_loader, optimizer, criterion)\n",
        "    return acc\n",
        "\n",
        "# 1ë‹¨ê³„ ì‹¤í–‰\n",
        "study1 = optuna.create_study(direction=\"maximize\")\n",
        "study1.optimize(objective_step1, n_trials=25) # ì‹œë„íšŸìˆ˜\n",
        "\n",
        "best_lr = study1.best_params['lr']\n",
        "best_batch_size = study1.best_params['batch_size']\n",
        "best_weight_decay = study1.best_params['weight_decay']\n",
        "print(f\"1ë‹¨ê³„ ì™„ë£Œ, ìµœì ì˜ lr: {best_lr}, batch_size: {best_batch_size}, weight_decay: {best_weight_decay}\")"
      ],
      "metadata": {
        "id": "oPQsfR2HT8Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_step2(trial):\n",
        "    # 1ë‹¨ê³„ì—ì„œ ì°¾ì€ ìµœì ê°’ ê³ ì •\n",
        "    lr = best_lr\n",
        "    batch_size = best_batch_size\n",
        "    weight_decay = best_weight_decay\n",
        "    # ğŸ” í•„í„° ìˆ˜(ì±„ë„ ìˆ˜) íƒìƒ‰ ê³µê°„ ì„¤ì •\n",
        "    # 16ë¶€í„° 128ê¹Œì§€ 16 ë‹¨ìœ„ë¡œ íƒìƒ‰\n",
        "    n_filters_conv1 = trial.suggest_int(\"n_filters_conv1\", 32, 64, step=16)\n",
        "    n_filters_conv2 = trial.suggest_int(\"n_filters_conv2\", 64, 128, step=16)\n",
        "    n_filters_conv3 = trial.suggest_int(\"n_filters_conv3\", 128, 256, step=16)\n",
        "    dropout_p = trial.suggest_float(\"dropout_p\", 0.2, 0.5)\n",
        "\n",
        "    # ë°ì´í„° ë¡œë”\n",
        "    train_loader = DataLoader(train_color_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # ëª¨ë¸ ìƒì„± (í•„í„° ìˆ˜ë¥¼ ì¸ìë¡œ ì „ë‹¬)\n",
        "    model = ColorCNN(\n",
        "        conv1_filters=n_filters_conv1,\n",
        "        conv2_filters=n_filters_conv2,\n",
        "        conv3_filters=n_filters_conv3,\n",
        "        dropout_p=dropout_p\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=best_weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # í•™ìŠµ ë° ì„±ì  ì¸¡ì •\n",
        "    acc = train_and_eval(model, train_loader, test_loader, optimizer, criterion)\n",
        "    return acc\n",
        "\n",
        "# 2ë‹¨ê³„ ì‹¤í–‰\n",
        "study2 = optuna.create_study(direction=\"maximize\")\n",
        "study2.optimize(objective_step2, n_trials=15)\n",
        "\n",
        "print(f\"2ë‹¨ê³„ ì™„ë£Œ, ìµœì ì˜ êµ¬ì¡°: {study2.best_params}\")"
      ],
      "metadata": {
        "id": "irMiCDtaVexK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = study2.best_params\n",
        "\n",
        "# 2. ê·¸ ë ˆì‹œí”¼ëŒ€ë¡œ 'ì§„ì§œ ìµœì¢… ëª¨ë¸'ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "final_model = ColorCNN(\n",
        "    conv1_filters=best_params['n_filters_conv1'],\n",
        "    conv2_filters=best_params['n_filters_conv2'],\n",
        "    conv3_filters=best_params['n_filters_conv3'],\n",
        "    dropout_p=best_params['dropout_p']\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(train_color_dataset, batch_size=best_batch_size, shuffle=True)\n",
        "optimizer = optim.Adam(final_model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"ìµœì¢… ëª¨ë¸ êµ¬ì„± ì™„ë£Œ\")\n",
        "\n",
        "history = {'train_loss': [], 'val_acc': []}\n",
        "\n",
        "epochs = 30\n",
        "for epoch in range(epochs):\n",
        "    final_model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = final_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # ì—í­ë§ˆë‹¤ ê²€ì¦ (í…ŒìŠ¤íŠ¸ì…‹ í™œìš©)\n",
        "    final_model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = final_model(images)\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            correct += (pred == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct / len(test_color_dataset)\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    history['train_loss'].append(avg_loss)\n",
        "    history['val_acc'].append(acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}, Acc: {acc:.2f}%\")\n",
        "\n",
        "print(\"ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "32oSE_1OZbs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(model, test_loader, classes):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    # ìˆ˜ì¹˜ í‘œì‹œ\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "plot_confusion_matrix(final_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "Xso9oK1_j7pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì‹¬í™” CNN"
      ],
      "metadata": {
        "id": "Tlof5WImh-fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet\n",
        "VGGNet\n",
        "EfficientNet"
      ],
      "metadata": {
        "id": "Rmj2Hw_eiA_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ìì—°ì–´ ì²˜ë¦¬"
      ],
      "metadata": {
        "id": "MMiXm-wjftAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "train_data = pd.read_table('ratings_train.txt')\n",
        "\n",
        "print(f\"ì „ì²´ ë¦¬ë·° ê°œìˆ˜: {len(train_data)}\")\n",
        "\n",
        "train_data = train_data.drop_duplicates(subset=['document'])\n",
        "train_data['document'] = train_data['document'].str.replace(\"[^ã„±-ã…ã…-ã…£ê°€-í£ ]\", \"\", regex=True)\n",
        "\n",
        "train_data['document'] = train_data['document'].replace('', np.nan)\n",
        "train_data = train_data.dropna(how='any')\n",
        "\n",
        "print(f\"ì •ì œ í›„ ë¦¬ë·° ê°œìˆ˜: {len(train_data)}\")\n",
        "\n",
        "stopwords = ['ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤']\n",
        "new_words = ['ì˜í™”', 'ì œ', 'ë³´ëŠ”', 'ë‚˜', 'ë‚´', 'ê±°', 'ì—ì„œ'] # ë‚´ ë°ì´í„°ì—ë§Œ ìœ ë… ë§ì€ ë‹¨ì–´\n",
        "stopwords.extend(new_words)\n",
        "\n",
        "okt = Okt()\n",
        "X_train = []\n",
        "\n",
        "for sentence in train_data['document']:\n",
        "    # í† í°í™” + ì–´ê°„ ì¶”ì¶œ(Stemming)\n",
        "    temp_X = okt.morphs(sentence, stem=True)\n",
        "    # ë¶ˆìš©ì–´ ì œê±°\n",
        "    temp_X = [word for word in temp_X if not word in stopwords]\n",
        "    X_train.append(temp_X)\n",
        "\n",
        "print(X_train[0])"
      ],
      "metadata": {
        "id": "BdU2GIcpfylG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ratings_train.pkl', 'wb') as f:\n",
        "    pickle.dump(X_train, f)\n",
        "\n",
        "print(\"í† í°í™” ì™„ë£Œ ë°ì´í„° ì €ì¥\")"
      ],
      "metadata": {
        "id": "Fq19fy4CKT6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ratings_train.pkl', 'rb') as f:\n",
        "    X_train = pickle.load(f)\n",
        "\n",
        "print(f\"ë°ì´í„° ë¡œë“œ\")"
      ],
      "metadata": {
        "id": "D5SgQnTbMjjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¦¬ë·° ì „ì²´ì˜ ë‹¨ì–´ ì‚¬ì „\n",
        "all_words = np.hstack(X_train)\n",
        "word_counts = Counter(all_words) # {'ì¬ë°Œë‹¤': 12345, 'ìµœì•…': 876, 'ê°ë…': 542}\n",
        "\n",
        "threshold = 3\n",
        "#ë¹ˆë„ ìˆ˜ ë‚®ì€ ë‹¨ì–´ ë²„ë¦¬ê¸°\n",
        "vocab = [word for word, count in word_counts.items() if count >= threshold] # ['ì¬ë°Œë‹¤', 'ìµœì•…', 'ê°ë…']\n",
        "#ë”•ì…”ë„ˆë¦¬ (ë‹¨ì–´ : ì¸ë±ìŠ¤) ì¸ë±ìŠ¤ ë§¤ê¸°ê¸°\n",
        "word_to_index = {word: i+2 for i, word in enumerate(vocab)} # [(0, 'ì¬ë°Œë‹¤'), (1, 'ìµœì•…'), (2, 'ê°ë…')]\n",
        "word_to_index['<PAD>'] = 0\n",
        "word_to_index['<UNK>'] = 1\n",
        "\n",
        "def encode_text(token_list):\n",
        "    return [word_to_index.get(word, 1) for word in token_list]\n",
        "\n",
        "X_train_encoded = [encode_text(sentence) for sentence in X_train]"
      ],
      "metadata": {
        "id": "0U1kjiFoMtC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# íŒ¨ë”© êµ¬í˜„\n",
        "def pad_sequences(sentences, max_len):\n",
        "    # ë¦¬ë·°ê°œìˆ˜ X ìµœëŒ€ ê¸¸ì´\n",
        "    features = np.zeros((len(sentences), max_len), dtype=int)\n",
        "    for i, sms in enumerate(sentences):\n",
        "        if len(sms) != 0:\n",
        "            # ë¬¸ì¥ì´ max_lenë³´ë‹¤ ê¸¸ë©´ ìë¥´ê³ , ì§§ìœ¼ë©´ ì•ì— 0(Padding)ì´ ìœ ì§€ë¨\n",
        "            features[i, -len(sms):] = np.array(sms)[:max_len]\n",
        "    return features\n",
        "\n",
        "max_len = 30 # ë¦¬ë·°ì˜ ì ë‹¹í•œ ê¸¸ì´\n",
        "X_train_padded = pad_sequences(X_train_encoded, max_len)\n",
        "\n",
        "print(f\"íŒ¨ë”© ì™„ë£Œëœ ì²« ë²ˆì§¸ ë¦¬ë·°: \\n{X_train_padded[0]}\")"
      ],
      "metadata": {
        "id": "nHWyN4oWPKxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° ì…‹ìœ¼ë¡œ ë³€í™˜\n",
        "y_train = torch.LongTensor(train_data['label'].values)\n",
        "X_train_tensor = torch.LongTensor(X_train_padded)"
      ],
      "metadata": {
        "id": "Bb9eoBSsPVOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# + LSTM"
      ],
      "metadata": {
        "id": "yfpAwqFrQkPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasetê³¼ DataLoader ìƒì„±\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "aaHmhr2NQit7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # 1. ì„ë² ë”© ì¸µ: ìˆ«ìë¥¼ ì˜ë¯¸ ìˆëŠ” ë²¡í„°ë¡œ ë³€í™˜\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # 2. LSTM ì¸µ: ë¬¸ì¥ì˜ ë§¥ë½ì„ íŒŒì•…\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        # 3. ì „ê²°í•© ì¸µ: ìµœì¢… ê¸ì •(1)/ë¶€ì •(0) íŒë‹¨\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        # LSTMì˜ ì¶œë ¥ ì¤‘ ë§ˆì§€ë§‰ ì‹œì ì˜ ê²°ê³¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        return self.fc(hidden[-1])"
      ],
      "metadata": {
        "id": "2p1UG3vUQrVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì¥ì¹˜: {device}\")\n",
        "\n",
        "vocab_size = len(word_to_index)\n",
        "\n",
        "# 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "embedding_dim = 100  # ë‹¨ì–´ë¥¼ í‘œí˜„í•  ë²¡í„°ì˜ ì°¨ì›\n",
        "hidden_dim = 128     # LSTMì˜ ë©”ëª¨ë¦¬ í¬ê¸°\n",
        "output_dim = 2       # 0(ë¶€ì •) ë˜ëŠ” 1(ê¸ì •)\n",
        "learning_rate = 0.001\n",
        "epochs = 5\n",
        "\n",
        "# 2. ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ìµœì í™” ë„êµ¬ ì„ ì–¸\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss() # ë¶„ë¥˜ ë¬¸ì œì´ë¯€ë¡œ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì‚¬ìš©\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam ìµœì í™”\n",
        "\n",
        "# 3. í•™ìŠµ ë£¨í”„ ì‹œì‘\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ìˆœì „íŒŒ (Forward)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # ì—­ì „íŒŒ (Backward) ë° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # í†µê³„ ê³„ì‚°\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    print(f'Epoch [{epoch+1}/{epochs}] - Loss: {total_loss/len(train_loader):.4f}, Acc: {acc:.2f}%')"
      ],
      "metadata": {
        "id": "VJgIMMMmQtxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì‹¤ì „\n",
        "def predict_sentiment(model, word_to_index, sentence):\n",
        "    model.eval()\n",
        "    # 1. ì „ì²˜ë¦¬: í•œê¸€ë§Œ ë‚¨ê¸°ê¸°\n",
        "    import re\n",
        "    sentence = re.sub(r'[^ã„±-ã…ã…-ã…£ê°€-í£ ]', '', sentence)\n",
        "\n",
        "    # 2. í† í°í™” ë° ë¶ˆìš©ì–´ ì œê±° (ì•„ê¹Œ ì‚¬ìš©í•œ oktì™€ stopwords í™œìš©)\n",
        "    tokens = okt.morphs(sentence, stem=True)\n",
        "    tokens = [word for word in tokens if word not in stopwords]\n",
        "\n",
        "    # 3. ì •ìˆ˜ ì¸ì½”ë”© ë° íŒ¨ë”©\n",
        "    encoded = [word_to_index.get(word, 1) for word in tokens] # ì‚¬ì „ì— ì—†ìœ¼ë©´ <UNK>(1)\n",
        "    padded = pad_sequences([encoded], max_len=30) # ìœ„ì—ì„œ ì •ì˜í•œ pad_sequences í•¨ìˆ˜ ì‚¬ìš©\n",
        "\n",
        "    # 4. ëª¨ë¸ ì˜ˆì¸¡\n",
        "    inputs = torch.LongTensor(padded).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        # í™•ë¥ ë¡œ ë³€í™˜ (Softmax)\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        score = probs[0][1].item() # ê¸ì •ì¼ í™•ë¥ \n",
        "\n",
        "    if score > 0.5:\n",
        "        print(f\"[{sentence}] -> ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤. (í™•ë¥ : {score*100:.2f}%)\")\n",
        "    else:\n",
        "        print(f\"[{sentence}] -> ë¶€ì • ë¦¬ë·°ì…ë‹ˆë‹¤. (í™•ë¥ : {(1-score)*100:.2f}%)\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ\n",
        "predict_sentiment(model, word_to_index, \"ì™€ ì´ ì˜í™” ì§„ì§œ ì†Œë¦„ ë‹ì„ ì •ë„ë¡œ ì¬ë°Œë„¤ìš”!\")\n",
        "predict_sentiment(model, word_to_index, \"ì‹œê°„ ì•„ê¹Œì›Œìš”. ë³´ì§€ ë§ˆì„¸ìš”.\")"
      ],
      "metadata": {
        "id": "AszNMiLBRzI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "oVsLlzWYd5Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•œêµ­ì–´ ì „ë¬¸ BERT ëª¨ë¸ (Beomiì˜ KcBERT)\n",
        "# ì „ì²˜ë¦¬\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
        "\n",
        "# 2. ë¡œë“œ (ë¶ˆí•„ìš”í•œ ê²°ì¸¡ì¹˜ ì œê±°ê¹Œì§€ í¬í•¨í•˜ëŠ” ê²Œ ì¢‹ìŠµë‹ˆë‹¤)\n",
        "train_df = pd.read_table('ratings_train.txt').dropna()\n",
        "test_df = pd.read_table('ratings_test.txt').dropna()\n",
        "\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42)\n",
        "\n",
        "# ìƒ˜í”Œë§í•´ì„œ ì¡°ê¸ˆë§Œ ì‚¬ìš©\n",
        "#train_df = train_df.sample(n=10000, random_state=42)\n",
        "#val_df = val_df.sample(n=2000, random_state=42)\n",
        "#test_df = test_df.sample(n=2000, random_state=42)\n",
        "\n",
        "#ëª¨ë¸ ë‹¤ë¥¸ê±°ë¡œ ë³€ê²½ ê°€ëŠ¥\n",
        "model_name = \"beomi/kcbert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 3. ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
        "def tokenize_function(examples):\n",
        "    # max_lengthëŠ” GPU ë©”ëª¨ë¦¬ì— ë§ì¶° ì¡°ì ˆ (ë³´í†µ 64, 128 ì‚¬ìš©)\n",
        "    return tokenizer(examples[\"document\"], padding=\"max_length\", truncation=True, max_length=64)\n",
        "\n",
        "# 4. HuggingFace Datasetìœ¼ë¡œ ë³€í™˜\n",
        "train_dataset = Dataset.from_pandas(train_df).map(tokenize_function, batched=True)\n",
        "val_dataset = Dataset.from_pandas(val_df).map(tokenize_function, batched=True)\n",
        "test_dataset = Dataset.from_pandas(test_df).map(tokenize_function, batched=True)\n",
        "\n",
        "# 5. BERT ì „ìš© í˜•ì‹ìœ¼ë¡œ ì •ë¦¬ (í•„ìš”ì—†ëŠ” ì»¬ëŸ¼ ì‚­ì œ ë° ì´ë¦„ ë³€ê²½)\n",
        "train_dataset = train_dataset.remove_columns(['document'])\n",
        "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
        "val_dataset = val_dataset.remove_columns(['document'])\n",
        "val_dataset = val_dataset.rename_column(\"label\", \"labels\")\n",
        "test_dataset = test_dataset.remove_columns(['document'])\n",
        "test_dataset = test_dataset.rename_column(\"label\", \"labels\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "I4ryDeS8d78O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# 2. í•™ìŠµ ì„¤ì • (ì—í­, ë°°ì¹˜ í¬ê¸° ë“±)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # ê²°ê³¼ ì €ì¥ ê²½ë¡œ\n",
        "\n",
        "    num_train_epochs=3,              # ì—í­ ìˆ˜ (ë³´í†µ 2~5)\n",
        "    learning_rate=2e-5,              # 2e-5(0.00002) ~ 5e-5(0.00005)\n",
        "    per_device_train_batch_size=16,  # ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ 8ë¡œ ì¤„ì„)\n",
        "    per_device_eval_batch_size=16,\n",
        "\n",
        "    warmup_steps=500,                # í•™ìŠµ ì´ˆê¸° ì•ˆì •í™”ë¥¼ ìœ„í•œ ë‹¨ê³„ (ë°ì´í„°ê°€ ìˆ˜ë§Œê°œ : 1000 ì´ìƒ)\n",
        "    weight_decay=0.01,               # ê³¼ì í•© ë°©ì§€\n",
        "    logging_dir='./logs',            # ë¡œê·¸ ì €ì¥ ê²½ë¡œ\n",
        "    eval_strategy=\"epoch\",           # ì—í­ë§ˆë‹¤ ì„±ëŠ¥ ê²€ì¦\n",
        "    save_strategy=\"epoch\",           # ì—í­ë§ˆë‹¤ ëª¨ë¸ ì €ì¥\n",
        "    load_best_model_at_end=True      # ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì„ ë§ˆì§€ë§‰ì— ë¡œë“œ\n",
        ")\n",
        "\n",
        "# 3. í•™ìŠµ ì‹œì‘! (ì´ê²Œ ë°”ë¡œ Fine-tuningì…ë‹ˆë‹¤)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "AwcK51HohyGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í…ŒìŠ¤íŠ¸\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "# 2. ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜ ì •ì˜\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # metric.computeë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•ë„ ê³„ì‚°\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# 3. Trainerì— í•¨ìˆ˜ ì£¼ì… ë° í‰ê°€\n",
        "trainer.compute_metrics = compute_metrics\n",
        "final_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "print(final_results)\n",
        "\n",
        "# ì‹¤ì œ ë¬¸ì¥ í…ŒìŠ¤íŠ¸\n",
        "# í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ê°ì„± ë¶„ì„ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì—°ê²°\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0)\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸í•´ë³¼ ë¬¸ì¥ë“¤\n",
        "sentences = [\n",
        "    \"ì˜í™” ì§„ì§œ ê°œê¿€ì¼ ã…‹ã…‹ã…‹ ê¼­ ë³´ì„¸ìš”\",\n",
        "    \"ë‚´ ì‹œê°„ ëŒë ¤ì¤˜... ì§„ì§œ ë…¸ì¼ì„\",\n",
        "    \"ì²˜ìŒì—” ì¢€ ì§€ë£¨í–ˆëŠ”ë° ëìœ¼ë¡œ ê°ˆìˆ˜ë¡ ì†Œë¦„ ë‹ë„¤ìš”\"\n",
        "]\n",
        "\n",
        "results = classifier(sentences)\n",
        "\n",
        "for i, res in enumerate(results):\n",
        "    if res['label'] == 'LABEL_1' :\n",
        "      label = \"ê¸ì •\"\n",
        "    else :\n",
        "      label = \"ë¶€ì •\"\n",
        "    print(f\"ë¦¬ë·°: {sentences[i]} => ê²°ê³¼: {label} (ì‹ ë¢°ë„: {res['score']:.2f})\")"
      ],
      "metadata": {
        "id": "l6sMk7pymRl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT + optuna\n"
      ],
      "metadata": {
        "id": "vPfvnLjKpLZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
        "\n",
        "# 2. ë¡œë“œ (ë¶ˆí•„ìš”í•œ ê²°ì¸¡ì¹˜ ì œê±°ê¹Œì§€ í¬í•¨í•˜ëŠ” ê²Œ ì¢‹ìŠµë‹ˆë‹¤)\n",
        "train_df = pd.read_table('ratings_train.txt').dropna()\n",
        "test_df = pd.read_table('ratings_test.txt').dropna()\n",
        "\n",
        "# íŒŒë¼ë¯¸í„° ì°¾ê¸°ìš© ë°ì´í„°\n",
        "train_sample = train_df.sample(n=20000, random_state=42)\n",
        "test_sample = test_df.sample(n=5000, random_state=42)\n",
        "\n",
        "#ëª¨ë¸ ë‹¤ë¥¸ê±°ë¡œ ë³€ê²½ ê°€ëŠ¥\n",
        "model_name = \"beomi/kcbert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 3. ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
        "def tokenize_function(examples):\n",
        "    # max_lengthëŠ” GPU ë©”ëª¨ë¦¬ì— ë§ì¶° ì¡°ì ˆ (ë³´í†µ 64, 128 ì‚¬ìš©)\n",
        "    return tokenizer(examples[\"document\"], padding=\"max_length\", truncation=True, max_length=64)\n",
        "\n",
        "train_optuna = Dataset.from_pandas(train_sample).map(tokenize_function, batched=True)\n",
        "test_optuna = Dataset.from_pandas(test_sample).map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df).map(tokenize_function, batched=True)\n",
        "test_dataset = Dataset.from_pandas(test_df).map(tokenize_function, batched=True)\n",
        "\n",
        "columns_to_remove = ['document', 'id'] if 'id' in train_df.columns else ['document']\n",
        "\n",
        "# Optunaìš© ë°ì´í„° ì •ë¦¬\n",
        "train_optuna = train_optuna.remove_columns(columns_to_remove)\n",
        "train_optuna = train_optuna.rename_column(\"label\", \"labels\")\n",
        "test_optuna = test_optuna.remove_columns(columns_to_remove)\n",
        "test_optuna = test_optuna.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# ì „ì²´ ë°ì´í„° ì •ë¦¬ (ìµœì¢… í•™ìŠµìš©)\n",
        "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
        "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
        "test_dataset = test_dataset.remove_columns(columns_to_remove)\n",
        "test_dataset = test_dataset.rename_column(\"label\", \"labels\")"
      ],
      "metadata": {
        "id": "umRQ78DHsIUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # ê²°ê³¼ ì €ì¥ ê²½ë¡œ\n",
        "\n",
        "    #num_train_epochs=3,              # ì—í­ ìˆ˜ (ë³´í†µ 2~5)\n",
        "    #learning_rate=2e-5,              # 2e-5(0.00002) ~ 5e-5(0.00005)\n",
        "    #per_device_train_batch_size=16,  # ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ 8ë¡œ ì¤„ì„)\n",
        "    per_device_eval_batch_size=16,\n",
        "\n",
        "    warmup_steps=500,                # í•™ìŠµ ì´ˆê¸° ì•ˆì •í™”ë¥¼ ìœ„í•œ ë‹¨ê³„ (ë°ì´í„°ê°€ ìˆ˜ë§Œê°œ : 1000 ì´ìƒ)\n",
        "    #warmup_ratio=0.1 # ì´ê±° ì‚¬ìš©í•˜ê¸°ë„ í•¨\n",
        "    weight_decay=0.01,               # ê³¼ì í•© ë°©ì§€\n",
        "    logging_dir='./logs',            # ë¡œê·¸ ì €ì¥ ê²½ë¡œ\n",
        "    eval_strategy=\"epoch\",           # ì—í­ë§ˆë‹¤ ì„±ëŠ¥ ê²€ì¦\n",
        "    save_strategy=\"epoch\",           # ì—í­ë§ˆë‹¤ ëª¨ë¸ ì €ì¥\n",
        "    load_best_model_at_end=True      # ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì„ ë§ˆì§€ë§‰ì— ë¡œë“œ\n",
        ")\n",
        "\n",
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # ë§ˆì§€ë§‰ ì„ íƒì§€ ê°œìˆ˜\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # metric.computeë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•ë„ ê³„ì‚°\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# 2. Trainer ì¬ì •ì˜ (ê¸°ì¡´ ì„¤ì • í™œìš©)\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,             # model ëŒ€ì‹  model_init ì‚¬ìš©\n",
        "    args=training_args,                # ì•„ê¹Œ ë§Œë“  training_args ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
        "    train_dataset=train_optuna,\n",
        "    eval_dataset=test_optuna,\n",
        "    compute_metrics=compute_metrics    # ì•„ê¹Œ ë§Œë“  ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜\n",
        ")\n",
        "\n",
        "# 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë²”ìœ„ ì •ì˜ í•¨ìˆ˜\n",
        "def my_hp_space(trial):\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
        "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 3),\n",
        "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32]),\n",
        "    }\n",
        "\n",
        "# 4. íƒìƒ‰ ì‹œì‘!\n",
        "best_run = trainer.hyperparameter_search(\n",
        "    direction=\"maximize\",      # ì •í™•ë„ë¥¼ ìµœëŒ€í™”í•˜ë¼\n",
        "    backend=\"optuna\",          # Optuna ì—”ì§„ ì‚¬ìš©\n",
        "    hp_space=my_hp_space,      # ë²”ìœ„ ì§€ì •\n",
        "    n_trials=5                 # 5ë²ˆ ì‹œë„ (BERTëŠ” ë¬´ê±°ìš°ë‹ˆ ì ê²Œ ì‹œì‘í•˜ì„¸ìš”)\n",
        ")\n",
        "\n",
        "print(\"ìµœê³ ì˜ íŒŒë¼ë¯¸í„°:\", best_run.hyperparameters)"
      ],
      "metadata": {
        "id": "EqEnkW8JpU6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_training_args = TrainingArguments(\n",
        "    output_dir='./final_results',\n",
        "    num_train_epochs=best_run.hyperparameters['num_train_epochs'],\n",
        "    learning_rate=best_run.hyperparameters['learning_rate'],\n",
        "    per_device_train_batch_size=best_run.hyperparameters['per_device_train_batch_size'],\n",
        "    per_device_eval_batch_size=best_run.hyperparameters['per_device_train_batch_size'], # ë™ì¼í•˜ê²Œ ì„¤ì •\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",            # ì—í­ë§ˆë‹¤ ì €ì¥ ì‹œë„ëŠ” í•˜ë˜\n",
        "    save_total_limit=1,               # ğŸ‘ˆ ë”± 1ê°œë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì§€ì›Œë¼!\n",
        "    load_best_model_at_end=True,      # ğŸ‘ˆ ê°€ì¥ ì ìˆ˜ ì¢‹ì€ ë†ˆì„ ë§ˆì§€ë§‰ì— ë¶ˆëŸ¬ì™€ë¼\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")\n",
        "\n",
        "# 3. ìµœì¢… í•™ìŠµìš© Trainer ìƒì„±\n",
        "final_trainer = Trainer(\n",
        "    model=AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2),\n",
        "    args=final_training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# 4. ì§„ì§œ ë§ˆì§€ë§‰ í•™ìŠµ!\n",
        "final_trainer.train()"
      ],
      "metadata": {
        "id": "uM_M8Zk-rvJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_score = final_trainer.evaluate()\n",
        "print(f\"ìµœì¢… ì •í™•ë„: {final_score['eval_accuracy']:.4f}\")\n",
        "\n",
        "# ëª¨ë¸ ì €ì¥\n",
        "final_trainer.save_model(\"./my_best_bert_model\")\n",
        "tokenizer.save_pretrained(\"./my_best_bert_model\")\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/Colab Notebooks/models/review_bert\"\n",
        "\n",
        "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥\n",
        "final_trainer.save_model(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"ëª¨ë¸ì´ êµ¬ê¸€ ë“œë¼ì´ë¸Œì˜ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
      ],
      "metadata": {
        "id": "gHnDAH6ItlYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "save_path = \"/content/drive/MyDrive/Colab Notebooks/models/review_bert\"\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ\n",
        "#device = torch.device(\"cpu\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(save_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "text = \"ë³„ë¡œì„\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "with torch.no_grad(): # ì¶”ë¡  ì‹œì—ëŠ” ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ í•„ìˆ˜!\n",
        "    outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "prediction = torch.argmax(logits, dim=-1) # ê°€ì¥ í° ê°’ì˜ ì¸ë±ìŠ¤(0 ë˜ëŠ” 1) ì¶”ì¶œ\n",
        "\n",
        "if prediction == 1:\n",
        "    print(\"ê¸ì •ì…ë‹ˆë‹¤!\")\n",
        "else:\n",
        "    print(\"ë¶€ì •ì…ë‹ˆë‹¤!\")"
      ],
      "metadata": {
        "id": "tIBnTW0RFkBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ëª¨ë¸ ì €ì¥ / ë¶ˆëŸ¬ì˜¤ê¸°"
      ],
      "metadata": {
        "id": "IGCs2gKS048R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê°€ì¤‘ì¹˜ë§Œ ì €ì¥í•˜ë¯€ë¡œ ëª¨ë¸ êµ¬ì¡°(í´ë˜ìŠ¤ ì½”ë“œ)ë„ ì €ì¥í•´ì•¼í•¨\n",
        "\n",
        "# ì €ì¥\n",
        "torch.save(final_model.state_dict(), 'cifar10_cnn_model.pth')\n",
        "print(\"ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ ì™„ë£Œ\")\n",
        "\n",
        "# ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "new_model = MNIST_CNN().to(device)\n",
        "\n",
        "new_model.load_state_dict(torch.load('mnist_cnn.pth'))\n",
        "\n",
        "# 3. í‰ê°€ ëª¨ë“œë¡œ ì „í™˜ (ë°˜ë“œì‹œ í•„ìš”!)\n",
        "new_model.eval()\n",
        "print(\"ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "l802uOnT09WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥\n",
        "model.save_pretrained(\"./my_bert_model\")\n",
        "tokenizer.save_pretrained(\"./my_bert_model\")\n",
        "\n",
        "\n",
        "# ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "model_path = \"./my_bert_model\"\n",
        "\n",
        "# 1. ì €ì¥ëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# 2. ì‹¤ì „ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ íŒŒì´í”„ë¼ì¸ ë§Œë“¤ê¸°\n",
        "# (device=0ì€ GPU ì‚¬ìš©, CPUë§Œ ìˆë‹¤ë©´ ìƒëµí•˜ê±°ë‚˜ -1ë¡œ ì„¤ì •)\n",
        "classifier = pipeline(\"sentiment-analysis\", model=loaded_model, tokenizer=loaded_tokenizer, device=0)"
      ],
      "metadata": {
        "id": "k1ENDMRoovlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì‹œê°í™”"
      ],
      "metadata": {
        "id": "UlTzawq6IisV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ì–´ë–¤ íŒŒë¼ë¯¸í„°ê°€ ê°€ì¥ ì¤‘ìš”í–ˆë‚˜?\n",
        "vis.plot_param_importances(study2)\n",
        "\n",
        "# 2. ìµœì í™” ê³¼ì • ê·¸ë˜í”„ (ì •í™•ë„ê°€ ì–´ë–»ê²Œ ìƒìŠ¹í–ˆë‚˜)\n",
        "vis.plot_optimization_history(study2)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "# ì†ì‹¤ ê·¸ë˜í”„\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.title('Loss Trend')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "# ì •í™•ë„ ê·¸ë˜í”„\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['val_acc'], label='Val Accuracy', color='orange')\n",
        "plt.title('Accuracy Trend')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3-dfyu6UIthA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ìˆ˜í•™"
      ],
      "metadata": {
        "id": "OnpL5ZWDU0jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„°ì˜ í‰ê· , í‘œì¤€í¸ì°¨ êµ¬í•˜ê¸°\n",
        "\n",
        "def get_mean_std(dataset):\n",
        "    # ë°ì´í„°ë¥¼ í•œ ë°°ì¹˜ì”© êº¼ë‚´ì˜¤ê¸° ìœ„í•œ ë¡œë” (ì…”í”Œ í•„ìš” ì—†ìŒ)\n",
        "    loader = DataLoader(dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "    mean = 0.\n",
        "    std = 0.\n",
        "    total_images = 0\n",
        "\n",
        "    for images, _ in loader:\n",
        "        # images: [batch_size, 3, height, width]\n",
        "        batch_size = images.size(0)\n",
        "\n",
        "        # 1. í”½ì…€ë“¤ì„ ì±„ë„ë³„ë¡œ í¼ì¹˜ê¸°: [batch_size, 3, H*W]\n",
        "        images = images.view(batch_size, images.size(1), -1)\n",
        "\n",
        "        # 2. ì±„ë„ë³„ í‰ê·  ê³„ì‚° (dim=2ì— ëŒ€í•´ í‰ê· )\n",
        "        mean += images.mean(2).sum(0)\n",
        "\n",
        "        # 3. ì±„ë„ë³„ í‘œì¤€í¸ì°¨ ê³„ì‚°\n",
        "        std += images.std(2).sum(0)\n",
        "\n",
        "        total_images += batch_size\n",
        "\n",
        "    # 4. ì „ì²´ ì´ë¯¸ì§€ ìˆ˜ë¡œ ë‚˜ëˆ„ê¸°\n",
        "    mean /= total_images\n",
        "    std /= total_images\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "# ì‚¬ìš© ì˜ˆì‹œ (Normalizeê°€ ì ìš©ë˜ì§€ ì•Šì€ ToTensorë§Œ ìˆëŠ” ë°ì´í„°ì…‹ í•„ìš”)\n",
        "print(get_mean_std(train_dataset))"
      ],
      "metadata": {
        "id": "05n3DgQGU2ku"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}